---
title: "PSET5: Webscraping"
author: "Jakub Budz & Charles Huang"
date: "November 9, 2024"
format:
  pdf:
    code-overflow: wrap
execute:
  eval: true 
  echo: true
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Jakub Budz jbudz1):
    - Partner 2 (Charles Huang chuang2):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_JB\_\*\* \*\*\_CH\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_0\_\*\* Late coins left after submission: \*\*\_JB:2, CH:1\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from bs4 import BeautifulSoup
import requests
import warnings 
warnings.filterwarnings('ignore')
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

Title of the enforcement action
Date
Category (e.g, “Criminal and Civil Actions”)
Link associated with the enforcement action

```{python}
# Import url into soup object
url = 'https://oig.hhs.gov/fraud/enforcement'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
```

```{python}
# Create df
# Attribution: ChatGPT was used to get general understanding of classes & bs4
# functions. With, I was able to pull all of the enforcement actions into an
# object "actions," which made the scraping function much easier to apply.
data = []
actions = soup.find_all(
    'li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
for action in actions:
# Title
    title_tag = action.find('h2', class_='usa-card__heading')
    title = title_tag.get_text(strip=True) if title_tag else None
# Date
    date_tag = action.find('span', class_='text-base-dark')
    date = date_tag.get_text(strip=True) if date_tag else None
# Category
    category_tag = action.find(
        'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_tag.get_text(strip=True) if category_tag else None
# Link
    link = title_tag.find(
        'a')['href'] if title_tag and title_tag.find('a') else None
    if link and not link.startswith("http"):
        link = "https://oig.hhs.gov" + link
    data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link
    })
oig_data = pd.DataFrame(data)
print(oig_data.head())
```

### 2. Crawling (PARTNER 1)

```{python}
# Define function that captures Agency names
# Attribution: ChatGPT assisted with the text stripping
# Also used StackOverflow for general guidance:
# https://stackoverflow.com/questions/68076739/
# creating-a-function-for-my-python-web-scraper-that-will-output-a-dictionary
def retrieve_agency(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'lxml')
    li_tags = soup.find_all('li')

    for li in li_tags:
        span_tag = li.find('span', class_='padding-right-2 text-base')
        if span_tag and span_tag.text.strip() == "Agency:":
            agency_name = li.get_text(
                strip=True).replace("Agency:", "").strip()
            return agency_name

oig_data['Agency Name'] = oig_data['Link'].apply(lambda x: retrieve_agency(x))
print(oig_data.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```