---
title: "PSET5: Webscraping"
author: "Jakub Budz & Charles Huang"
date: "November 9, 2024"
format:
  pdf:
    code-overflow: wrap
execute:
  eval: true 
  echo: true
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Jakub Budz jbudz1):
    - Partner 2 (Charles Huang chuang2):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_JB\_\*\* \*\*\_CH\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_0\_\*\* Late coins left after submission: \*\*\_JB:2, CH:1\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from bs4 import BeautifulSoup
import requests
import warnings 
import lxml
warnings.filterwarnings('ignore')
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

Title of the enforcement action
Date
Category (e.g, “Criminal and Civil Actions”)
Link associated with the enforcement action

```{python}
# Import url into soup object
url = 'https://oig.hhs.gov/fraud/enforcement'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
```

```{python}
# Create df
# Attribution: ChatGPT was used to get general understanding of classes & bs4
# functions. With, I was able to pull all of the enforcement actions into an
# object "actions," which made the scraping function much easier to apply.
data = []
actions = soup.find_all(
    'li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')
for action in actions:
# Title
    title_tag = action.find('h2', class_='usa-card__heading')
    title = title_tag.get_text(strip=True) if title_tag else None
# Date
    date_tag = action.find('span', class_='text-base-dark')
    date = date_tag.get_text(strip=True) if date_tag else None
# Category
    category_tag = action.find(
        'li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
    category = category_tag.get_text(strip=True) if category_tag else None
# Link
    link = title_tag.find(
        'a')['href'] if title_tag and title_tag.find('a') else None
    if link and not link.startswith("http"):
        link = "https://oig.hhs.gov" + link
    data.append({
        'Title': title,
        'Date': date,
        'Category': category,
        'Link': link
    })
oig_data = pd.DataFrame(data)
print(oig_data.head())
```

### 2. Crawling (PARTNER 1)

```{python}
# Define function that captures Agency names
# Attribution: ChatGPT assisted with the text stripping
# Also used StackOverflow for general guidance:
# https://stackoverflow.com/questions/68076739/
# creating-a-function-for-my-python-web-scraper-that-will-output-a-dictionary
def retrieve_agency(link):
    response = requests.get(link)
    soup = BeautifulSoup(response.text, 'lxml')
    li_tags = soup.find_all('li')

    for li in li_tags:
        span_tag = li.find('span', class_='padding-right-2 text-base')
        if span_tag and span_tag.text.strip() == "Agency:":
            agency_name = li.get_text(
                strip=True).replace("Agency:", "").strip()
            return agency_name

oig_data['Agency Name'] = oig_data['Link'].apply(lambda x: retrieve_agency(x))
print(oig_data.head())
```



## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

Turning the scraper into a function: You will write a function that takes as input a month and a year, and then pulls and formats the enforcement actions like in Step 1 starting from that month+year to today.

• This function should first check that the year inputted >= 2013 before starting to scrape. If the year inputted < 2013, it should print a statement reminding the user to restrict to year >= 2013, since only enforcement actions after 2013 are listed.
• It should save the dataframe output into a .csv file named as “enforcement_actions_year_month.csv” (do not commit this file to git)
• If you’re crawling multiple pages, always add 1 second wait before going to the next page to prevent potential server-side block. To implement this in Python, you may look up .sleep() function from time library.

Before writing out your function, write down pseudo-code of the steps that your function will go through. If you use a loop, discuss what kind of loop you will use and how you will define it.

def scraper_year_check(month, year):
    if year < 2013:
        print an error statement
        return
    else:
        my_data = []
        convert target month and year to a datetime object called target_date
        set current_page = 1

        while True (keep going until loop)
            set URL to "https://oig.hhs.gov/fraud/enforcement/?page={current_page}"
            soup = fetch the page and make a soup object
            li_tags = use find_all on soup to collect all li tags
            data_found = boolean track if entry matching the date range found

            for each li in li_tags:
                find the date from the span tag and set it to date_text
                compare date_text to the target_date
                if date_text < target date:
                    break the while loop (done collecting data)

                if loop not broken:

                find title
                find category
                find link
                append these to my_data as a dictionary
                time.sleep(1) #pauses for 1 second to prevent server block
                get the next page URL if available
        convert my_data into a dataframe called oig_dataframe
        oig_dataframe.to_csv(f"enforcement_actions_{year}_{month}.csv")

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from datetime import datetime

def scraper_year_check(month, year):
    if year < 2013:
        print("Year must be 2013 or later. Please enter a valid year.")
        return

    my_data = []
    target_date = datetime(year, month, 1)
    current_page = 1

    while True:
        url = f"https://oig.hhs.gov/fraud/enforcement/?page={current_page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'lxml')

        li_tags = soup.find_all(
            'li', class_='usa-card card--list pep-card--minimal mobile:grid-col-12')

        data_found = False

        for li in li_tags:
            date_text = li.find('span', class_="text-base-dark padding-right-105").get_text(
                strip=True) if li.find('span', class_='text-base-dark padding-right-105') else None

            if date_text:
                date_object = datetime.strptime(date_text, "%B %d, %Y")

                if date_object < target_date:
                    oig_data = pd.DataFrame(my_data)
                    filename = f"enforcement_actions_{year}_{month}.csv"
                    oig_data.to_csv(filename, index=False)
                    print(f"Data saved to {filename}")
                    return

                data_found = True

                title = li.find(
                    'h2', class_="usa-card__heading").get_text(strip=True) if li.find('h2') else None
                category = li.find('li', class_='display-inline-block').get_text(
                    strip=True) if li.find('li', class_='display-inline-block') else None
                link = li.find('a')['href'] if li.find('a') else None
                if link and not link.startswith("http"):
                    link = "https://oig.hhs.gov" + link

                my_data.append({
                    'Title': title,
                    'Date': date_text,
                    'Category': category,
                    'Link': link
                })
        # stop the loop if no data found
        if not data_found:
            break

        time.sleep(1)

        current_page += 1

scraper_year_check(1, 2023)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
scraper_year_check(1, 2021)
oig_2021 = pd.read_csv('enforcement_actions_2021_1.csv')

oig_2021['Agency Name'] = oig_2021['Link'].apply(lambda x: retrieve_agency(x))
print(f'There are {len(oig_2021)} observations since 2021.')
oig_2021['Date'] = pd.to_datetime(oig_2021['Date'])
print(oig_2021.loc[oig_2021['Date'].idxmin()])

```

```{python}

#Note: This code does the same thing as the above chunk but should run faster

#Attribution: Used ChatGPT to figure out how to speed up the original code, since retrieve_agency was taking incredibly long. ChatGPT suggested using an asynchronous function with these associated packages, as well as turning off SSL checks (this is public non-sensitive data so should be OK.)

import asyncio
import aiohttp
from bs4 import BeautifulSoup

import nest_asyncio
nest_asyncio.apply()

import ssl
import aiohttp

# Create an SSL context to ignore verification
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

async def retrieve_agency_async(session, link):
    async with session.get(link, ssl=ssl_context) as response:  
        html = await response.text()
        soup = BeautifulSoup(html, 'lxml')
        li_tags = soup.find_all('li')

        for li in li_tags:
            span_tag = li.find('span', class_='padding-right-2 text-base')
            if span_tag and span_tag.text.strip() == "Agency:":
                agency_name = li.get_text(strip=True).replace("Agency:", "").strip()
                return agency_name
    return None

# Define a function to process all links asynchronously
async def fetch_all_agencies(links):
    async with aiohttp.ClientSession() as session:
        tasks = [retrieve_agency_async(session, link) for link in links]
        return await asyncio.gather(*tasks)

# Use the asynchronous function in place of the original apply
links = oig_2021['Link'].unique() 
agency_names = asyncio.run(fetch_all_agencies(links))

# Create a dictionary to map links to agency names for faster lookup
agency_dict = dict(zip(links, agency_names))
oig_2021['Agency Name'] = oig_2021['Link'].map(agency_dict)


```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

(Partner 2) Plot a line chart that shows: the number of enforcement actions over time (aggregated to each month+year) overall since January 2021.

```{python}

import numpy as np

oig_2021['Date'] = pd.to_datetime(oig_2021['Date'])

oig_2021_summary = oig_2021.groupby(oig_2021['Date'].dt.to_period('M')).size().reset_index(name='Count')

# Convert the 'Date' column back to datetime format
oig_2021_summary['Date'] = oig_2021_summary['Date'].dt.to_timestamp()

enf_actions = alt.Chart(oig_2021_summary).mark_line().encode(
    x=alt.X('Date:T', axis=alt.Axis(format='%Y-%m', title='Date', labelAngle=-45)),
    y=alt.Y('Count:Q', title='Count of Enforcement Actions')
).properties(title="Monthly HHS Enforcement Actions from 2021 Onwards")

enf_actions


```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

Among actions taken by US Attorney District-level agencies, clean the district names so that you can merge them with the shapefile, and then plot a choropleth of the number of enforcement actions in each US Attorney District.
Hint: look for “District” in the agency info.

(We need to take the oig_2021 data and subset it to actions taken by US Attorney District level agencies only. Then clean the names and plot them with the shapefile data.)

```{python}
import geopandas as gpd
import re

oig_2021_district = oig_2021[oig_2021['Agency Name'].str.contains('District', na=False)]
district_data = gpd.read_file("/Users/charleshuang/Downloads/US Attorney Districts Shapefile simplified_20241108/geo_export_9d536c74-a84f-4bce-b9ee-76d1ad9c0104.shp")
district_data.head()

#Attribution: Used ChatGPT for help to clean the data. Searched for "how to clean text string so that it removes all text before a specified symbol such as a comma or colon". Used the split() function to do this

def clean_agency(agency_name):
    parts = re.split(r'[,:;]', agency_name)
    return parts[-1].strip() if parts else agency_name

oig_2021_district['Agency Name'] = oig_2021_district['Agency Name'].apply(clean_agency)

#Need to group the data by district:

district_counts = oig_2021_district.groupby('Agency Name').size().reset_index(name="Action Count")

#Now we merge the district data with the shapefile:

merged_districts = district_counts.merge(district_data, left_on="Agency Name", right_on="judicial_d", how='left')
merged_districts = gpd.GeoDataFrame(merged_districts, geometry="geometry")


```

Finally we can plot the choropleth:


```{python}

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(15, 10))

merged_districts.plot(
    column="Action Count", cmap="Blues", linewidth=0.8, ax=ax,legend=True,legend_kwds={
        "shrink": 0.5,       
        "orientation": "vertical"
    }
)

# Set xlim and ylim to focus on the contiguous U.S. (continental 48 states) to avoid map drift
ax.set_xlim([-130, -60]) 
ax.set_ylim([20, 55])   

ax.set_aspect('equal')

ax.set_axis_off()
ax.set_title("Number of HHS Actions per Judicial District", fontsize=16, pad=20)

plt.show()


```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```